{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Numpy library for performing mathematical calculations\n",
    "import numpy as np\n",
    "\n",
    "#matplotlib for plot functions\n",
    "from matplotlib import pyplot as plt\n",
    "\t\n",
    "#nltk library to import the treebank dataset\n",
    "import nltk\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n",
    "\n",
    "print(nltk_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading every sentence into X and Y variables\n",
    "def load_sentences(tagged_sentences):\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for sentence in tagged_sentences:\n",
    "        X_sentence = []\n",
    "        Y_sentence = []\n",
    "        for word in sentence:         \n",
    "            X_sentence.append(word[0]) #word[0] contains the word\n",
    "            Y_sentence.append(word[1]) #word[1] contains the corresponding tag\n",
    "        \n",
    "        X.append(X_sentence)\n",
    "        Y.append(Y_sentence)\n",
    "    \n",
    "    return X,Y\n",
    "\n",
    "X_train, Y_train = load_sentences(nltk_data)\n",
    "\n",
    "print(\"Total number of tagged sentences: {}\".format(len(X_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#Encoding X\n",
    "word_tokenizer = Tokenizer()            \n",
    "word_tokenizer.fit_on_texts(X_train)     \n",
    "\n",
    "#Using the tokenizer to encode the input sequence\n",
    "X_encoded = word_tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "#Encoding Y\n",
    "tag_tokenizer = Tokenizer()\n",
    "tag_tokenizer.fit_on_texts(Y_train)\n",
    "Y_encoded = tag_tokenizer.texts_to_sequences(Y_train)\n",
    "\n",
    "#Look at first encoded data point\n",
    "print(\"Original data:\", \"\\n\",)\n",
    "print('X: ', X_train[0], '\\n')\n",
    "print('Y: ', Y_train[0], '\\n')\n",
    "print(\"Encoded data:\", \"\\n\")\n",
    "print('X: ', X_encoded[0], '\\n')\n",
    "print('Y: ', Y_encoded[0], '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "#Path to word2vec model\n",
    "path = '../input/googlenewsvectors/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "\n",
    "#Assign word vectors from the pre trained word2vec model\n",
    "\n",
    "EMBEDDING_SIZE  = 300  #Every word in word2vec model is represented using a 300 dimensional vector\n",
    "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "#Empty embedding matrix\n",
    "embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n",
    "\n",
    "#Word to index dictionary mapping\n",
    "word2id = word_tokenizer.word_index\n",
    "\n",
    "#Copying the vectors from word2vec model to the words present in our corpus\n",
    "for word, index in word2id.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = word2vec[word]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "print(\"Embeddings shape: {}\".format(embedding_weights.shape))\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# use Keras' to_categorical function to one-hot encode Y\n",
    "Y_train = to_categorical(Y_train)\n",
    "\n",
    "# print Y of the first output sequence\n",
    "print(\"Shape of Y: {}\".format(Y_train.shape))\n",
    "\n",
    "X,Y = X_train,Y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=0.2, random_state=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
    "\n",
    "#Creating the architecutre\n",
    "\n",
    "rnn_model = Sequential()\n",
    "\n",
    "#Creating the embedding layer\n",
    "#It is usually the first layer in any text problem\n",
    "rnn_model.add(Embedding(input_dim = VOCABULARY_SIZE, \n",
    "                        output_dim = EMBEDDING_SIZE,          \n",
    "                        input_length = MAX_SEQ_LENGTH,          \n",
    "                        trainable = False                    \n",
    "))\n",
    "\n",
    "#Add an RNN layer with 64 RNN cells\n",
    "rnn_model.add(SimpleRNN(64, \n",
    "              return_sequences = True  # True - Return whole sequence\n",
    "))\n",
    "\n",
    "#Add an output after each sequence\n",
    "rnn_model.add(TimeDistributed(Dense(NUM_CLASSES, activation='softmax')))\n",
    "\n",
    "rnn_model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = 'adamax',\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "#Summary of the model\n",
    "rnn_model.summary()\n",
    "\n",
    "rnn_training = rnn_model.fit(X_train, Y_train, batch_size=32, epochs=20, \n",
    "validation_data=(X_validation, Y_validation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = Sequential()\n",
    "\n",
    "#Embedding layer\n",
    "rnn_model.add(Embedding(input_dim = VOCABULARY_SIZE,         \n",
    "                        output_dim = EMBEDDING_SIZE,          \n",
    "                        input_length = MAX_SEQ_LENGTH,          \n",
    "                        trainable = True                     \n",
    "))\n",
    "\n",
    "#RNN layer\n",
    "rnn_model.add(SimpleRNN(64, \n",
    "              return_sequences = True  \n",
    "))\n",
    "\n",
    "#Output after each sequence\n",
    "rnn_model.add(TimeDistributed(Dense(NUM_CLASSES, activation='softmax')))\n",
    "\n",
    "rnn_model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = 'adamax',\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "rnn_training = rnn_model.fit(X_train, Y_train, batch_size=32, epochs=20, \n",
    "validation_data=(X_validation, Y_validation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = Sequential()\n",
    "\n",
    "#Embedding layer\n",
    "rnn_model.add(Embedding(input_dim = VOCABULARY_SIZE,         \n",
    "                        output_dim = EMBEDDING_SIZE,          \n",
    "                        input_length = MAX_SEQ_LENGTH,\n",
    "                        weights = [embedding_weights],          \n",
    "                        trainable = True                     \n",
    "))\n",
    "\n",
    "#RNN layer\n",
    "rnn_model.add(SimpleRNN(64, \n",
    "              return_sequences = True  \n",
    "))\n",
    "\n",
    "#Output after each sequence\n",
    "rnn_model.add(TimeDistributed(Dense(NUM_CLASSES, activation='softmax')))\n",
    "\n",
    "rnn_model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = 'adamax',\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "rnn_model.summary()\n",
    "\n",
    "rnn_training = rnn_model.fit(X_train, Y_train, batch_size=32, epochs=20, \n",
    "validation_data=(X_validation, Y_validation))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
